{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc288f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Prevent OpenMP runtime conflicts (libomp vs libiomp) that crash the kernel\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "# Keep thread counts modest to reduce chance of runtime contention on Windows\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2577812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= CONFIGURATION =================\n",
    "INPUT_FOLDER = 'data/images_shift'\n",
    "OUTPUT_FOLDER = 'output/shadow_masks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27489e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLOv8 on 0...\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n-seg.pt to 'yolov8n-seg.pt': 100% ━━━━━━━━━━━━ 6.7MB 10.7MB/s 0.6s.6s<0.0s0s\n",
      "Generating Background Model...\n",
      "Processing 36 images...\n",
      "Saved: mask_IMG_20250906_110448_00_008.jpg\n",
      "Saved: mask_IMG_20250906_111350_00_016.jpg\n",
      "Saved: mask_IMG_20250906_112248_00_024.jpg\n",
      "Saved: mask_IMG_20250906_113147_00_032.jpg\n",
      "Saved: mask_IMG_20250906_114044_00_040.jpg\n",
      "Saved: mask_IMG_20250906_114942_00_048.jpg\n",
      "Saved: mask_IMG_20250906_115841_00_056.jpg\n",
      "Saved: mask_IMG_20250906_120744_00_064.jpg\n",
      "Saved: mask_IMG_20250906_121643_00_072.jpg\n",
      "Saved: mask_IMG_20250906_122542_00_080.jpg\n",
      "Saved: mask_IMG_20250906_123441_00_088.jpg\n",
      "Saved: mask_IMG_20250906_124340_00_096.jpg\n",
      "Saved: mask_IMG_20250906_125238_00_104.jpg\n",
      "Saved: mask_IMG_20250906_130139_00_112.jpg\n",
      "Saved: mask_IMG_20250906_131039_00_120.jpg\n",
      "Saved: mask_IMG_20250906_131937_00_128.jpg\n",
      "Saved: mask_IMG_20250906_132836_00_136.jpg\n",
      "Saved: mask_IMG_20250906_160131_00_143.jpg\n",
      "Saved: mask_IMG_20250906_161032_00_151.jpg\n",
      "Saved: mask_IMG_20250906_161930_00_159.jpg\n",
      "Saved: mask_IMG_20250906_162828_00_167.jpg\n",
      "Saved: mask_IMG_20250906_163726_00_175.jpg\n",
      "Saved: mask_IMG_20250906_164623_00_183.jpg\n",
      "Saved: mask_IMG_20250906_165520_00_191.jpg\n",
      "Saved: mask_IMG_20250906_170417_00_199.jpg\n",
      "Saved: mask_IMG_20250906_171318_00_207.jpg\n",
      "Saved: mask_IMG_20250906_172215_00_215.jpg\n",
      "Saved: mask_IMG_20250906_173112_00_223.jpg\n",
      "Saved: mask_IMG_20250906_174010_00_231.jpg\n",
      "Saved: mask_IMG_20250906_174907_00_239.jpg\n",
      "Saved: mask_IMG_20250906_175804_00_247.jpg\n",
      "Saved: mask_IMG_20250906_180701_00_255.jpg\n",
      "Saved: mask_IMG_20250906_181558_00_263.jpg\n",
      "Saved: mask_IMG_20250906_181558_00_263_preview.jpg\n",
      "Saved: mask_IMG_20250906_182456_00_271.jpg\n",
      "Saved: mask_IMG_20250906_183824_00_283.jpg\n",
      "Processing Complete.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# 1. SHADOW SETTINGS\n",
    "RATIO_THRESHOLD = 0.65\n",
    "ABSOLUTE_DARK_THRESHOLD = 40\n",
    "\n",
    "# 2. PERSON HANDLING\n",
    "USE_AI = True\n",
    "AI_CONFIDENCE = 0.3\n",
    "CONTEXT_RING_SIZE = 20\n",
    "FILL_THRESHOLD = 0.5\n",
    "\n",
    "# 3. CLEANUP\n",
    "SKY_RATIO = 0.45\n",
    "MIN_SHADOW_SIZE = 200 # Minimum pixel area to keep\n",
    "# =================================================\n",
    "\n",
    "# Pre-calculate kernels to save time\n",
    "KERNEL_DILATE = np.ones((5,5), np.uint8)\n",
    "KERNEL_MORPH = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "KERNEL_RING = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (CONTEXT_RING_SIZE, CONTEXT_RING_SIZE))\n",
    "\n",
    "def create_background_model(files):\n",
    "    print(\"Generating Background Model...\")\n",
    "    first_img = cv2.imread(files[0])\n",
    "    max_buffer = np.zeros_like(first_img)\n",
    "\n",
    "    for f in files:\n",
    "        img = cv2.imread(f)\n",
    "        if img is None: continue\n",
    "        max_buffer = np.maximum(max_buffer, img)\n",
    "\n",
    "    return max_buffer\n",
    "\n",
    "def get_person_mask(img, model):\n",
    "    h, w = img.shape[:2]\n",
    "    person_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "\n",
    "    # Force GPU usage with device=0\n",
    "    results = model(img, classes=[0], verbose=False, conf=AI_CONFIDENCE, device=0)\n",
    "\n",
    "    if results[0].masks is not None:\n",
    "        masks = results[0].masks.data.cpu().numpy()\n",
    "        for mask in masks:\n",
    "            mask_resized = cv2.resize(mask, (w, h))\n",
    "            person_mask = np.maximum(person_mask, (mask_resized * 255).astype(np.uint8))\n",
    "\n",
    "    person_mask = cv2.dilate(person_mask, KERNEL_DILATE, iterations=1)\n",
    "    return person_mask\n",
    "\n",
    "def segment_shadows_raw(img, bg_model):\n",
    "    # Convert to float32 for faster vectorized division\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY).astype(np.float32)\n",
    "    gray_bg = cv2.cvtColor(bg_model, cv2.COLOR_BGR2GRAY).astype(np.float32)\n",
    "\n",
    "    safe_bg = gray_bg.copy()\n",
    "    safe_bg[safe_bg == 0] = 1.0\n",
    "\n",
    "    # Vectorized operations\n",
    "    mask_ratio = (gray_img / safe_bg) < RATIO_THRESHOLD\n",
    "    mask_absolute = gray_img < ABSOLUTE_DARK_THRESHOLD\n",
    "\n",
    "    return np.logical_or(mask_ratio, mask_absolute).astype(np.uint8) * 255\n",
    "\n",
    "def fill_person_holes(shadow_mask, person_mask):\n",
    "    final_mask = shadow_mask.copy()\n",
    "\n",
    "    if person_mask is None or np.count_nonzero(person_mask) == 0:\n",
    "        return final_mask\n",
    "\n",
    "    contours, _ = cv2.findContours(person_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Loop is unavoidable here for context-aware logic, \n",
    "    # but efficient OpenCV calls minimize impact\n",
    "    for cnt in contours:\n",
    "        single_person_mask = np.zeros_like(shadow_mask)\n",
    "        cv2.drawContours(single_person_mask, [cnt], -1, 255, -1)\n",
    "\n",
    "        dilated_person = cv2.dilate(single_person_mask, KERNEL_RING)\n",
    "        ring_mask = cv2.subtract(dilated_person, single_person_mask)\n",
    "\n",
    "        shadow_pixels_in_ring = cv2.bitwise_and(ring_mask, shadow_mask)\n",
    "\n",
    "        total_ring_pixels = cv2.countNonZero(ring_mask)\n",
    "        if total_ring_pixels == 0: continue\n",
    "        \n",
    "        shadow_ring_pixels = cv2.countNonZero(shadow_pixels_in_ring)\n",
    "\n",
    "        if (shadow_ring_pixels / total_ring_pixels) > FILL_THRESHOLD:\n",
    "            final_mask = cv2.bitwise_or(final_mask, single_person_mask)\n",
    "        else:\n",
    "            final_mask = cv2.bitwise_and(final_mask, cv2.bitwise_not(single_person_mask))\n",
    "\n",
    "    return final_mask\n",
    "\n",
    "def post_process_fast(mask, h):\n",
    "    \"\"\"\n",
    "    Optimized cleanup using Vectorized NumPy operations.\n",
    "    \"\"\"\n",
    "    # 1. Mask Sky\n",
    "    sky_limit = int(h * SKY_RATIO)\n",
    "    mask[:sky_limit, :] = 0\n",
    "\n",
    "    # 2. Clean Noise\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, KERNEL_MORPH)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, KERNEL_MORPH)\n",
    "\n",
    "    # 3. Filter Small Regions (VECTORIZED - HIGH SPEED)\n",
    "    if MIN_SHADOW_SIZE > 0:\n",
    "        # Get components\n",
    "        num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(mask, connectivity=8)\n",
    "\n",
    "        # stats[:, 4] is the Area column\n",
    "        sizes = stats[:, cv2.CC_STAT_AREA]\n",
    "\n",
    "        # Identify small blobs (True = remove, False = keep)\n",
    "        # We must make sure Background (index 0) is False, or we'll delete the canvas\n",
    "        small_blobs = sizes < MIN_SHADOW_SIZE\n",
    "        small_blobs[0] = False \n",
    "\n",
    "        # This line is the magic speedup.\n",
    "        # \"labels\" maps every pixel to an ID.\n",
    "        # \"small_blobs\" maps every ID to True/False.\n",
    "        # \"small_blobs[labels]\" generates a boolean mask for the whole image in one go.\n",
    "        mask[small_blobs[labels]] = 0\n",
    "\n",
    "    return mask\n",
    "\n",
    "# ================= MAIN LOOP =================\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(OUTPUT_FOLDER):\n",
    "        os.makedirs(OUTPUT_FOLDER)\n",
    "\n",
    "    files = sorted(glob.glob(os.path.join(INPUT_FOLDER, \"*.jpg\")))\n",
    "    if not files:\n",
    "        print(\"No images found.\")\n",
    "        exit()\n",
    "\n",
    "    # 1. Load AI (Check for GPU)\n",
    "    model = None\n",
    "    if USE_AI:\n",
    "        device = 0 if torch.cuda.is_available() else 'cpu'\n",
    "        print(f\"Loading YOLOv8 on {device}...\")\n",
    "        model = YOLO('yolov8n-seg.pt')\n",
    "\n",
    "    # 2. Background Model\n",
    "    bg_model = create_background_model(files)\n",
    "    h, w = bg_model.shape[:2]\n",
    "    cv2.imwrite(os.path.join(OUTPUT_FOLDER, \"background_ref.jpg\"), bg_model)\n",
    "\n",
    "    print(f\"Processing {len(files)} images...\")\n",
    "\n",
    "    for f in files:\n",
    "        base_name = os.path.basename(f)\n",
    "        img = cv2.imread(f)\n",
    "\n",
    "        # A. Get Person Mask\n",
    "        p_mask = None\n",
    "        if USE_AI:\n",
    "            p_mask = get_person_mask(img, model)\n",
    "\n",
    "        # B. Get Raw Shadows\n",
    "        raw_shadows = segment_shadows_raw(img, bg_model)\n",
    "\n",
    "        # C. Remove People\n",
    "        if p_mask is not None:\n",
    "            shadows_no_people = cv2.bitwise_and(raw_shadows, cv2.bitwise_not(p_mask))\n",
    "        else:\n",
    "            shadows_no_people = raw_shadows\n",
    "\n",
    "        # D. Intelligent Fill\n",
    "        if p_mask is not None:\n",
    "            final_mask = fill_person_holes(shadows_no_people, p_mask)\n",
    "        else:\n",
    "            final_mask = shadows_no_people\n",
    "\n",
    "        # E. Fast Post Process\n",
    "        final_mask = post_process_fast(final_mask, h)\n",
    "\n",
    "        cv2.imwrite(os.path.join(OUTPUT_FOLDER, f\"mask_{base_name}\"), final_mask)\n",
    "        print(f\"Saved: mask_{base_name}\")\n",
    "\n",
    "    print(\"Processing Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2050c918",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'INPUT_FOLDER' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 67\u001b[0m\n\u001b[0;32m     64\u001b[0m         plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Run the display\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m display_results(\u001b[43mINPUT_FOLDER\u001b[49m, OUTPUT_FOLDER, limit\u001b[38;5;241m=\u001b[39mNUM_SAMPLES, resize_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'INPUT_FOLDER' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "# ================= CONFIGURATION =================\n",
    "# INPUT_FOLDER = 'images'     # Folder with original images\n",
    "# OUTPUT_FOLDER = 'output_masks_people'    # Folder with generated masks\n",
    "NUM_SAMPLES = 36                   # How many pairs to display\n",
    "# =================================================\n",
    "\n",
    "def display_results(input_dir, output_dir, limit=5, resize_factor=1.0):\n",
    "    # Get list of original images\n",
    "    input_files = sorted(glob.glob(os.path.join(input_dir, \"*\")))\n",
    "\n",
    "    # Filter for image extensions only\n",
    "    input_files = [f for f in input_files if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    if not input_files:\n",
    "        print(\"No images found to display.\")\n",
    "        return\n",
    "\n",
    "    # Loop through the desired number of samples\n",
    "    for i, file_path in enumerate(input_files[:limit]):\n",
    "        filename = os.path.basename(file_path)\n",
    "\n",
    "        # Construct path to corresponding mask\n",
    "        # (Assuming the naming convention from previous script: 'mask_filename')\n",
    "        mask_path = os.path.join(output_dir, f\"mask_{filename}\")\n",
    "\n",
    "        if not os.path.exists(mask_path):\n",
    "            print(f\"Mask not found for: {filename}\")\n",
    "            continue\n",
    "\n",
    "        # Load images\n",
    "        # 1. Original: Read as BGR, Convert to RGB for Matplotlib\n",
    "        original = cv2.imread(file_path)\n",
    "        original = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # 2. Mask: Read as Grayscale\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Resize images if resize_factor is less than 1.0\n",
    "        if resize_factor < 1.0:\n",
    "            h, w = original.shape[:2]\n",
    "            original = cv2.resize(original, (int(w * resize_factor), int(h * resize_factor)), interpolation=cv2.INTER_AREA)\n",
    "            mask = cv2.resize(mask, (int(w * resize_factor), int(h * resize_factor)), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        # Plotting\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(20, 5)) # Wide layout for 360 images\n",
    "\n",
    "        # Original Image\n",
    "        axes[0].imshow(original)\n",
    "        axes[0].set_title(f\"Original: {filename}\")\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        # Generated Mask\n",
    "        axes[1].imshow(mask, cmap='gray')\n",
    "        axes[1].set_title(\"Generated Shadow Mask\")\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Run the display\n",
    "display_results(INPUT_FOLDER, OUTPUT_FOLDER, limit=NUM_SAMPLES, resize_factor=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52302406",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<=' not supported between instances of 'int' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpanogeo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmapplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mapping_shadow\n\u001b[1;32m----> 3\u001b[0m saved \u001b[38;5;241m=\u001b[39m \u001b[43mmapping_shadow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput/shadow_masks/*.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# list | dir | glob | file path\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcalibration_npz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput/calibration_cam2enu.npz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput/shadow_maps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprovider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjapan_gsi_air\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                      \u001b[49m\u001b[38;5;66;43;03m# e.g., \"carto\", \"osm\", \"japan_gsi_air\"\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwidth_m\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheight_m\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcell_size_m\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                               \u001b[49m\u001b[38;5;66;43;03m# surface mesh resolution\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrass_mask_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mkunih\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mOneDrive\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m00_Codes\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mpython\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mpanogeo\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mgrass_ground_mask.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdem_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m                                 \u001b[49m\u001b[38;5;66;43;03m# or DEM path\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m#000000\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdpi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_stride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstamp_timestamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     18\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kunih\\miniconda3\\envs\\panogeo\\lib\\site-packages\\panogeo\\mapplot.py:646\u001b[0m, in \u001b[0;36mmapping_shadow\u001b[1;34m(mask_paths, calibration_npz, out_dir, provider, zoom, width_m, height_m, cell_size_m, dem_path, grass_mask_path, alpha, color, dpi, pixel_stride, stamp_timestamp)\u001b[0m\n\u001b[0;32m    644\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_aspect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mequal\u001b[39m\u001b[38;5;124m\"\u001b[39m, adjustable\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbox\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    645\u001b[0m src \u001b[38;5;241m=\u001b[39m _get_provider(provider)\n\u001b[1;32m--> 646\u001b[0m \u001b[43mcx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_basemap\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEPSG:3857\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzoom\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzoom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattribution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_extent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    647\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_xlim(extent_merc[\u001b[38;5;241m0\u001b[39m], extent_merc[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    648\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_ylim(extent_merc[\u001b[38;5;241m2\u001b[39m], extent_merc[\u001b[38;5;241m3\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\kunih\\miniconda3\\envs\\panogeo\\lib\\site-packages\\contextily\\plotting.py:134\u001b[0m, in \u001b[0;36madd_basemap\u001b[1;34m(ax, zoom, source, interpolation, attribution, attribution_size, reset_extent, crs, resampling, zoom_adjust, **extra_imshow_args)\u001b[0m\n\u001b[0;32m    130\u001b[0m     left, right, bottom, top \u001b[38;5;241m=\u001b[39m _reproj_bb(\n\u001b[0;32m    131\u001b[0m         left, right, bottom, top, crs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepsg:3857\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    132\u001b[0m     )\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# Download image\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m image, extent \u001b[38;5;241m=\u001b[39m \u001b[43mbounds2img\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbottom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mzoom\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzoom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mll\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mzoom_adjust\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzoom_adjust\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m# Warping\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m crs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\kunih\\miniconda3\\envs\\panogeo\\lib\\site-packages\\contextily\\tile.py:269\u001b[0m, in \u001b[0;36mbounds2img\u001b[1;34m(w, s, e, n, zoom, source, ll, wait, max_retries, n_connections, use_cache, zoom_adjust)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m zoom_adjust:\n\u001b[0;32m    268\u001b[0m     zoom \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m zoom_adjust\n\u001b[1;32m--> 269\u001b[0m zoom \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_zoom\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzoom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_zoom\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;66;03m# create list of tiles to download\u001b[39;00m\n\u001b[0;32m    271\u001b[0m tiles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(mt\u001b[38;5;241m.\u001b[39mtiles(w, s, e, n, [zoom]))\n",
      "File \u001b[1;32mc:\\Users\\kunih\\miniconda3\\envs\\panogeo\\lib\\site-packages\\contextily\\tile.py:631\u001b[0m, in \u001b[0;36m_validate_zoom\u001b[1;34m(zoom, provider, auto)\u001b[0m\n\u001b[0;32m    628\u001b[0m     max_zoom \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n\u001b[0;32m    629\u001b[0m     max_zoom_known \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m min_zoom \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m zoom \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m max_zoom:\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m zoom\n\u001b[0;32m    634\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferred\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m auto \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecified\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: '<=' not supported between instances of 'int' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "from panogeo.mapplot import mapping_shadow\n",
    "\n",
    "saved = mapping_shadow(\n",
    "    mask_paths=\"output/shadow_masks/*.jpg\",           # list | dir | glob | file path\n",
    "    calibration_npz=\"output/calibration_cam2enu.npz\",\n",
    "    out_dir=\"output/shadow_maps\",\n",
    "    provider=\"japan_gsi_air\",                      # e.g., \"carto\", \"osm\", \"japan_gsi_air\"\n",
    "    width_m=50.0,\n",
    "    height_m=50.0,\n",
    "    cell_size_m=1.0,                               # surface mesh resolution\n",
    "    grass_mask_path=r\"C:\\Users\\kunih\\OneDrive\\00_Codes\\python\\panogeo\\output\\grass_ground_mask.png\",\n",
    "    dem_path=None,                                 # or DEM path\n",
    "    alpha=0.6,\n",
    "    color=\"#000000\",\n",
    "    dpi=150,\n",
    "    pixel_stride=1,\n",
    "    stamp_timestamp=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "panogeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
