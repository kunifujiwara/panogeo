{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q --upgrade ultralytics opencv-python numpy imutils tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d92076d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Track:\n",
    "    object_id: int\n",
    "    centroid: Tuple[int, int]\n",
    "    bbox: Tuple[int, int, int, int]\n",
    "    disappeared: int = 0\n",
    "\n",
    "\n",
    "class CentroidTracker:\n",
    "    def __init__(self, max_disappeared: int = 30, max_distance: float = 80.0):\n",
    "        self.next_object_id: int = 0\n",
    "        self.tracks: Dict[int, Track] = {}\n",
    "        self.max_disappeared = max_disappeared\n",
    "        self.max_distance = max_distance\n",
    "\n",
    "    def register(self, centroid: Tuple[int, int], bbox: Tuple[int, int, int, int]):\n",
    "        self.tracks[self.next_object_id] = Track(\n",
    "            object_id=self.next_object_id,\n",
    "            centroid=centroid,\n",
    "            bbox=bbox,\n",
    "            disappeared=0,\n",
    "        )\n",
    "        self.next_object_id += 1\n",
    "\n",
    "    def deregister(self, object_id: int):\n",
    "        if object_id in self.tracks:\n",
    "            del self.tracks[object_id]\n",
    "\n",
    "    def update(self, rects: List[Tuple[int, int, int, int]]) -> Dict[int, Track]:\n",
    "        # No detections: mark disappearances and remove stale tracks\n",
    "        if len(rects) == 0:\n",
    "            for object_id in list(self.tracks.keys()):\n",
    "                self.tracks[object_id].disappeared += 1\n",
    "                if self.tracks[object_id].disappeared > self.max_disappeared:\n",
    "                    self.deregister(object_id)\n",
    "            return dict(self.tracks)\n",
    "\n",
    "        # Compute input centroids\n",
    "        input_centroids = np.zeros((len(rects), 2), dtype=np.float32)\n",
    "        for i, (x1, y1, x2, y2) in enumerate(rects):\n",
    "            cX = int((x1 + x2) * 0.5)\n",
    "            cY = int((y1 + y2) * 0.5)\n",
    "            input_centroids[i] = (cX, cY)\n",
    "\n",
    "        # If no existing tracks, register all detections\n",
    "        if len(self.tracks) == 0:\n",
    "            for i, box in enumerate(rects):\n",
    "                self.register((int(input_centroids[i][0]), int(input_centroids[i][1])), box)\n",
    "            return dict(self.tracks)\n",
    "\n",
    "        # Build arrays of existing track centroids and ids\n",
    "        object_ids = list(self.tracks.keys())\n",
    "        object_centroids = np.array([self.tracks[oid].centroid for oid in object_ids], dtype=np.float32)\n",
    "\n",
    "        # Compute pairwise distances\n",
    "        D = np.linalg.norm(object_centroids[:, None, :] - input_centroids[None, :, :], axis=2)\n",
    "\n",
    "        # Greedy matching by minimum distance\n",
    "        rows = D.min(axis=1).argsort()\n",
    "        cols = D.argmin(axis=1)[rows]\n",
    "\n",
    "        used_rows = set()\n",
    "        used_cols = set()\n",
    "\n",
    "        for row, col in zip(rows, cols):\n",
    "            if row in used_rows or col in used_cols:\n",
    "                continue\n",
    "            if D[row, col] > self.max_distance:\n",
    "                continue\n",
    "\n",
    "            object_id = object_ids[row]\n",
    "            centroid = (int(input_centroids[col][0]), int(input_centroids[col][1]))\n",
    "            self.tracks[object_id].centroid = centroid\n",
    "            self.tracks[object_id].bbox = rects[col]\n",
    "            self.tracks[object_id].disappeared = 0\n",
    "\n",
    "            used_rows.add(row)\n",
    "            used_cols.add(col)\n",
    "\n",
    "        # Determine unused rows and columns\n",
    "        unused_rows = set(range(0, D.shape[0])).difference(used_rows)\n",
    "        unused_cols = set(range(0, D.shape[1])).difference(used_cols)\n",
    "\n",
    "        # For tracks that didn't get matched, mark disappeared\n",
    "        for row in unused_rows:\n",
    "            object_id = object_ids[row]\n",
    "            self.tracks[object_id].disappeared += 1\n",
    "            if self.tracks[object_id].disappeared > self.max_disappeared:\n",
    "                self.deregister(object_id)\n",
    "\n",
    "        # Register new detections for unmatched columns\n",
    "        for col in unused_cols:\n",
    "            centroid = (int(input_centroids[col][0]), int(input_centroids[col][1]))\n",
    "            self.register(centroid, rects[col])\n",
    "\n",
    "        return dict(self.tracks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1a59647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIDEO_PATH: data\\videos\\onikuru_cropped_mini.mp4\n",
      "MODEL_CANDIDATES: [WindowsPath('yolo12m.pt'), 'yolo12s.pt', 'yolo12n.pt', WindowsPath('yolo11m.pt'), WindowsPath('yolov8s.pt'), 'yolo11n.pt']\n",
      "OUTPUT_PATH: output\\people_track_20251204_212637\\_tmp_center_crop.mp4\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Paths\n",
    "VIDEO_PATH = Path(\"data/videos/onikuru_cropped_mini.mp4\")  # change if needed\n",
    "\n",
    "# Prefer YOLOv12 first, with graceful fallbacks\n",
    "MODEL_CANDIDATES = []\n",
    "for name in [\"yolo12m.pt\", \"yolo12s.pt\", \"yolo12n.pt\"]:\n",
    "    p = Path(name)\n",
    "    MODEL_CANDIDATES.append(p if p.exists() else name)\n",
    "for name in [\"yolo11m.pt\", \"yolov8s.pt\", \"yolo11n.pt\"]:\n",
    "    p = Path(name)\n",
    "    MODEL_CANDIDATES.append(p if p.exists() else name)\n",
    "\n",
    "STAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "OUT_DIR = Path(\"output\") / f\"people_track_{STAMP}\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_PATH = OUT_DIR / \"_tmp_center_crop.mp4\"\n",
    "\n",
    "# Detection/tracking parameters - tuned for small people\n",
    "CONF_THRES = 0.15\n",
    "IOU_THRES = 0.55\n",
    "IMG_SIZE = 1536           # larger resolution improves small-object recall\n",
    "MAX_DET = 1000\n",
    "PERSON_CLASS_ID = 0       # COCO person\n",
    "\n",
    "# Tracker params\n",
    "MAX_DISAPPEARED = 30\n",
    "MAX_DISTANCE = 90.0       # allow looser matches when subjects are tiny\n",
    "\n",
    "# Optional counting across a virtual line\n",
    "ENABLE_COUNTING = False\n",
    "LINE_Y_FRACTION = 0.55    # relative Y for line (e.g., 0.55 * frame_height)\n",
    "\n",
    "print(f\"VIDEO_PATH: {VIDEO_PATH}\")\n",
    "print(f\"MODEL_CANDIDATES: {MODEL_CANDIDATES}\")\n",
    "print(f\"OUTPUT_PATH: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48bcc71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CONF_THRES': 0.06, 'IOU_THRES': 0.4, 'IMG_SIZE': 1920, 'MAX_DET': 3000, 'AGNOSTIC_NMS': True, 'DEVICE': 0, 'TILING_2X2': True, 'TILE_OVERLAP': 160}\n"
     ]
    }
   ],
   "source": [
    "# 4K-focused overrides (tune for small people in 3840x2160)\n",
    "CONF_THRES = 0.06\n",
    "IOU_THRES = 0.4\n",
    "IMG_SIZE = 1920\n",
    "MAX_DET = 3000\n",
    "AGNOSTIC_NMS = True\n",
    "DEVICE = 0  # 0 for CUDA GPU, or \"cpu\"\n",
    "\n",
    "# Tracker tuning\n",
    "MAX_DISTANCE = 150.0\n",
    "\n",
    "# Tiling settings\n",
    "TILING_2X2 = True\n",
    "TILE_OVERLAP = 160  # pixels\n",
    "\n",
    "print({\n",
    "    \"CONF_THRES\": CONF_THRES,\n",
    "    \"IOU_THRES\": IOU_THRES,\n",
    "    \"IMG_SIZE\": IMG_SIZE,\n",
    "    \"MAX_DET\": MAX_DET,\n",
    "    \"AGNOSTIC_NMS\": AGNOSTIC_NMS,\n",
    "    \"DEVICE\": DEVICE,\n",
    "    \"TILING_2X2\": TILING_2X2,\n",
    "    \"TILE_OVERLAP\": TILE_OVERLAP,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f861daeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: yolo12m.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames: 100%|██████████| 615/615 [01:42<00:00,  6.02frame/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: output\\people_track_20251204_212637\\_tmp_center_crop.mp4\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize model (prefer YOLOv12)\n",
    "last_error = None\n",
    "model = None\n",
    "for candidate in MODEL_CANDIDATES:\n",
    "    try:\n",
    "        model = YOLO(str(candidate))\n",
    "        print(f\"Loaded model: {candidate}\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        last_error = e\n",
    "        print(f\"Failed to load {candidate}: {e}\")\n",
    "assert model is not None, f\"Could not load any model from {MODEL_CANDIDATES}. Last error: {last_error}\"\n",
    "\n",
    "# Open video\n",
    "cap = cv2.VideoCapture(str(VIDEO_PATH))\n",
    "assert cap.isOpened(), f\"Cannot open video: {VIDEO_PATH}\"\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Total frames (may be 0 for some codecs)\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "if total_frames <= 0:\n",
    "    total_frames = None\n",
    "\n",
    "# Writer\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "writer = cv2.VideoWriter(str(OUTPUT_PATH), fourcc, fps, (width, height))\n",
    "\n",
    "# Tracker and counts\n",
    "tracker = CentroidTracker(max_disappeared=MAX_DISAPPEARED, max_distance=MAX_DISTANCE)\n",
    "count_in, count_out = 0, 0\n",
    "line_y = int(LINE_Y_FRACTION * height)\n",
    "\n",
    "# Inference options tuned for small people\n",
    "predict_kwargs = dict(\n",
    "    conf=CONF_THRES,\n",
    "    iou=IOU_THRES,\n",
    "    imgsz=IMG_SIZE,\n",
    "    classes=[PERSON_CLASS_ID],  # person only\n",
    "    agnostic_nms=False,\n",
    "    max_det=MAX_DET,\n",
    "    augment=True,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "prev_centroids: Dict[int, Tuple[int, int]] = {}\n",
    "\n",
    "start_time = time.time()\n",
    "frame_count = 0\n",
    "\n",
    "pbar = tqdm(total=total_frames, desc=\"Processing frames\", unit=\"frame\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame_count += 1\n",
    "    pbar.update(1)\n",
    "\n",
    "    # Run YOLO inference\n",
    "    results = model.predict(frame, **predict_kwargs)\n",
    "    boxes_xyxy: List[Tuple[int, int, int, int]] = []\n",
    "    if len(results) > 0:\n",
    "        r = results[0]\n",
    "        if r.boxes is not None and len(r.boxes) > 0:\n",
    "            xyxy = r.boxes.xyxy.cpu().numpy().astype(int)\n",
    "            confs = r.boxes.conf.cpu().numpy()\n",
    "            clss = r.boxes.cls.cpu().numpy().astype(int)\n",
    "            for (x1, y1, x2, y2), c, k in zip(xyxy, confs, clss):\n",
    "                if k == PERSON_CLASS_ID and c >= CONF_THRES:\n",
    "                    # clamp to frame\n",
    "                    x1, y1 = max(0, x1), max(0, y1)\n",
    "                    x2, y2 = min(width - 1, x2), min(height - 1, y2)\n",
    "                    if x2 > x1 and y2 > y1:\n",
    "                        boxes_xyxy.append((x1, y1, x2, y2))\n",
    "\n",
    "    # Update tracker\n",
    "    tracks = tracker.update(boxes_xyxy)\n",
    "\n",
    "    # Optional counting (line-crossing)\n",
    "    if ENABLE_COUNTING:\n",
    "        for oid, tr in tracks.items():\n",
    "            cX, cY = tr.centroid\n",
    "            if oid in prev_centroids:\n",
    "                prevY = prev_centroids[oid][1]\n",
    "                # Upwards vs downwards crossing\n",
    "                if prevY < line_y <= cY:\n",
    "                    count_in += 1\n",
    "                elif prevY > line_y >= cY:\n",
    "                    count_out += 1\n",
    "            prev_centroids[oid] = (cX, cY)\n",
    "\n",
    "    # Draw\n",
    "    if ENABLE_COUNTING:\n",
    "        cv2.line(frame, (0, line_y), (width, line_y), (0, 255, 255), 2)\n",
    "        cv2.putText(frame, f\"IN: {count_in}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 200, 0), 2)\n",
    "        cv2.putText(frame, f\"OUT: {count_out}\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 200), 2)\n",
    "\n",
    "    for oid, tr in tracks.items():\n",
    "        x1, y1, x2, y2 = tr.bbox\n",
    "        cX, cY = tr.centroid\n",
    "        # draw bbox\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 140, 0), 2)\n",
    "        # draw centroid\n",
    "        cv2.circle(frame, (cX, cY), 3, (0, 255, 255), -1)\n",
    "        # id label\n",
    "        cv2.putText(frame, f\"ID {oid}\", (x1, max(0, y1 - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 140, 0), 2)\n",
    "\n",
    "    # FPS + progress overlay\n",
    "    elapsed = time.time() - start_time\n",
    "    fps_text = f\"FPS: {frame_count / elapsed:.1f}\" if elapsed > 0 else \"FPS: --\"\n",
    "    cv2.putText(frame, fps_text, (width - 180, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (50, 220, 50), 2)\n",
    "    prog_text = f\"{frame_count}/{total_frames}\" if total_frames else f\"{frame_count}\"\n",
    "    cv2.putText(frame, prog_text, (width - 180, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (200, 200, 50), 2)\n",
    "\n",
    "    writer.write(frame)\n",
    "\n",
    "pbar.close()\n",
    "cap.release()\n",
    "writer.release()\n",
    "\n",
    "print(f\"Saved: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7b74f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Center-crop processing: 100%|██████████| 615/615 [01:22<00:00,  7.48frame/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved center-crop video: output\\people_track_20251204_212637\\_tmp_center_crop.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Center-crop run (writes cropped annotated video)\n",
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure model is loaded\n",
    "try:\n",
    "    _ = model\n",
    "except NameError:\n",
    "    last_error = None\n",
    "    model = None\n",
    "    for candidate in MODEL_CANDIDATES:\n",
    "        try:\n",
    "            model = YOLO(str(candidate))\n",
    "            print(f\"Loaded model: {candidate}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            last_error = e\n",
    "            print(f\"Failed to load {candidate}: {e}\")\n",
    "    assert model is not None, f\"Could not load any model from {MODEL_CANDIDATES}. Last error: {last_error}\"\n",
    "\n",
    "# Open video\n",
    "cap = cv2.VideoCapture(str(VIDEO_PATH))\n",
    "assert cap.isOpened(), f\"Cannot open video: {VIDEO_PATH}\"\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "src_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "src_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Choose crop size: default to 1920x1080; change if you want\n",
    "CROP_W = min(1920, src_w)\n",
    "CROP_H = min(1080, src_h)\n",
    "\n",
    "# Center crop rectangle in source frame\n",
    "x0 = max(0, (src_w - CROP_W) // 2)\n",
    "y0 = max(0, (src_h - CROP_H) // 2)\n",
    "x1 = min(src_w, x0 + CROP_W)\n",
    "y1 = min(src_h, y0 + CROP_H)\n",
    "\n",
    "out_w, out_h = (x1 - x0), (y1 - y0)\n",
    "\n",
    "# Writer (cropped size)\n",
    "out_path = OUT_DIR / \"_tmp_center_crop.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "writer = cv2.VideoWriter(str(out_path), fourcc, fps, (out_w, out_h))\n",
    "\n",
    "# Tracker and counts\n",
    "tracker = CentroidTracker(max_disappeared=MAX_DISAPPEARED, max_distance=MAX_DISTANCE)\n",
    "count_in, count_out = 0, 0\n",
    "line_y = int(LINE_Y_FRACTION * out_h)\n",
    "\n",
    "predict_kwargs = dict(\n",
    "    conf=CONF_THRES,\n",
    "    iou=IOU_THRES,\n",
    "    imgsz=IMG_SIZE,\n",
    "    classes=[PERSON_CLASS_ID],\n",
    "    agnostic_nms=globals().get(\"AGNOSTIC_NMS\", False),\n",
    "    max_det=MAX_DET,\n",
    "    augment=True,\n",
    "    verbose=False,\n",
    "    device=globals().get(\"DEVICE\", 0),\n",
    "    half=True,\n",
    "    amp=True,\n",
    ")\n",
    "\n",
    "prev_centroids: Dict[int, Tuple[int, int]] = {}\n",
    "\n",
    "# Progress bar\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "if total_frames <= 0:\n",
    "    total_frames = None\n",
    "pbar = tqdm(total=total_frames, desc=\"Center-crop processing\", unit=\"frame\")\n",
    "\n",
    "start_time = time.time()\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame_count += 1\n",
    "    pbar.update(1)\n",
    "\n",
    "    crop = frame[y0:y1, x0:x1]\n",
    "\n",
    "    # Run YOLO inference on crop\n",
    "    results = model.predict(crop, **predict_kwargs)\n",
    "    boxes_xyxy: List[Tuple[int, int, int, int]] = []\n",
    "    if len(results) > 0:\n",
    "        r = results[0]\n",
    "        if r.boxes is not None and len(r.boxes) > 0:\n",
    "            xyxy = r.boxes.xyxy.cpu().numpy().astype(int)\n",
    "            confs = r.boxes.conf.cpu().numpy()\n",
    "            clss = r.boxes.cls.cpu().numpy().astype(int)\n",
    "            for (x1b, y1b, x2b, y2b), c, k in zip(xyxy, confs, clss):\n",
    "                if k == PERSON_CLASS_ID and c >= CONF_THRES:\n",
    "                    x1b, y1b = max(0, x1b), max(0, y1b)\n",
    "                    x2b, y2b = min(out_w - 1, x2b), min(out_h - 1, y2b)\n",
    "                    if x2b > x1b and y2b > y1b:\n",
    "                        boxes_xyxy.append((x1b, y1b, x2b, y2b))\n",
    "\n",
    "    # Update tracker with crop-space boxes\n",
    "    tracks = tracker.update(boxes_xyxy)\n",
    "\n",
    "    # Optional counting\n",
    "    if ENABLE_COUNTING:\n",
    "        for oid, tr in tracks.items():\n",
    "            cX, cY = tr.centroid\n",
    "            if oid in prev_centroids:\n",
    "                prevY = prev_centroids[oid][1]\n",
    "                if prevY < line_y <= cY:\n",
    "                    count_in += 1\n",
    "                elif prevY > line_y >= cY:\n",
    "                    count_out += 1\n",
    "            prev_centroids[oid] = (cX, cY)\n",
    "\n",
    "    # Draw overlays onto crop\n",
    "    if ENABLE_COUNTING:\n",
    "        cv2.line(crop, (0, line_y), (out_w, line_y), (0, 255, 255), 2)\n",
    "        cv2.putText(crop, f\"IN: {count_in}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 200, 0), 2)\n",
    "        cv2.putText(crop, f\"OUT: {count_out}\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 200), 2)\n",
    "\n",
    "    for oid, tr in tracks.items():\n",
    "        x1b, y1b, x2b, y2b = tr.bbox\n",
    "        cX, cY = tr.centroid\n",
    "        cv2.rectangle(crop, (x1b, y1b), (x2b, y2b), (255, 140, 0), 2)\n",
    "        cv2.circle(crop, (cX, cY), 3, (0, 255, 255), -1)\n",
    "        cv2.putText(crop, f\"ID {oid}\", (x1b, max(0, y1b - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 140, 0), 2)\n",
    "\n",
    "    # FPS + progress\n",
    "    elapsed = time.time() - start_time\n",
    "    fps_text = f\"FPS: {frame_count / elapsed:.1f}\" if elapsed > 0 else \"FPS: --\"\n",
    "    cv2.putText(crop, fps_text, (out_w - 180, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (50, 220, 50), 2)\n",
    "    prog_text = f\"{frame_count}/{total_frames}\" if total_frames else f\"{frame_count}\"\n",
    "    cv2.putText(crop, prog_text, (out_w - 180, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (200, 200, 50), 2)\n",
    "\n",
    "    writer.write(crop)\n",
    "\n",
    "pbar.close()\n",
    "cap.release()\n",
    "writer.release()\n",
    "\n",
    "print(f\"Saved center-crop video: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f640864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Full-frame w/ traj:  29%|██▉       | 179/615 [00:30<01:12,  6.00frame/s]"
     ]
    }
   ],
   "source": [
    "# Full-frame run with trajectory visualization\n",
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "\n",
    "# Ensure model is loaded\n",
    "try:\n",
    "    _ = model\n",
    "except NameError:\n",
    "    last_error = None\n",
    "    model = None\n",
    "    for candidate in MODEL_CANDIDATES:\n",
    "        try:\n",
    "            model = YOLO(str(candidate))\n",
    "            print(f\"Loaded model: {candidate}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            last_error = e\n",
    "            print(f\"Failed to load {candidate}: {e}\")\n",
    "    assert model is not None, f\"Could not load any model from {MODEL_CANDIDATES}. Last error: {last_error}\"\n",
    "\n",
    "# Open video\n",
    "cap = cv2.VideoCapture(str(VIDEO_PATH))\n",
    "assert cap.isOpened(), f\"Cannot open video: {VIDEO_PATH}\"\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Writer\n",
    "out_path_traj = OUT_DIR / \"_tmp_with_traj.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "writer = cv2.VideoWriter(str(out_path_traj), fourcc, fps, (width, height))\n",
    "\n",
    "# Tracker and counts\n",
    "tracker = CentroidTracker(max_disappeared=MAX_DISAPPEARED, max_distance=MAX_DISTANCE)\n",
    "count_in, count_out = 0, 0\n",
    "line_y = int(LINE_Y_FRACTION * height)\n",
    "\n",
    "# Trajectory store: per-id deque of centroids\n",
    "TRAJ_MAX_POINTS = 40\n",
    "TRAJ_THICKNESS = 2\n",
    "tracks_history: Dict[int, deque] = {}\n",
    "\n",
    "def id_color(oid: int) -> Tuple[int, int, int]:\n",
    "    # Deterministic color per id\n",
    "    r = (37 * oid) % 255\n",
    "    g = (17 * oid + 85) % 255\n",
    "    b = (97 * oid + 170) % 255\n",
    "    return int(b), int(g), int(r)\n",
    "\n",
    "# Inference options\n",
    "predict_kwargs = dict(\n",
    "    conf=CONF_THRES,\n",
    "    iou=IOU_THRES,\n",
    "    imgsz=IMG_SIZE,\n",
    "    classes=[PERSON_CLASS_ID],\n",
    "    agnostic_nms=globals().get(\"AGNOSTIC_NMS\", False),\n",
    "    max_det=MAX_DET,\n",
    "    augment=True,\n",
    "    verbose=False,\n",
    "    device=globals().get(\"DEVICE\", 0),\n",
    "    half=True,\n",
    "    amp=True,\n",
    ")\n",
    "\n",
    "prev_centroids: Dict[int, Tuple[int, int]] = {}\n",
    "\n",
    "# Progress bar\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "if total_frames <= 0:\n",
    "    total_frames = None\n",
    "pbar = tqdm(total=total_frames, desc=\"Full-frame w/ traj\", unit=\"frame\")\n",
    "\n",
    "start_time = time.time()\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame_count += 1\n",
    "    pbar.update(1)\n",
    "\n",
    "    # YOLO inference\n",
    "    results = model.predict(frame, **predict_kwargs)\n",
    "    boxes_xyxy: List[Tuple[int, int, int, int]] = []\n",
    "    if len(results) > 0:\n",
    "        r = results[0]\n",
    "        if r.boxes is not None and len(r.boxes) > 0:\n",
    "            xyxy = r.boxes.xyxy.cpu().numpy().astype(int)\n",
    "            confs = r.boxes.conf.cpu().numpy()\n",
    "            clss = r.boxes.cls.cpu().numpy().astype(int)\n",
    "            for (x1, y1, x2, y2), c, k in zip(xyxy, confs, clss):\n",
    "                if k == PERSON_CLASS_ID and c >= CONF_THRES:\n",
    "                    x1, y1 = max(0, x1), max(0, y1)\n",
    "                    x2, y2 = min(width - 1, x2), min(height - 1, y2)\n",
    "                    if x2 > x1 and y2 > y1:\n",
    "                        boxes_xyxy.append((x1, y1, x2, y2))\n",
    "\n",
    "    # Update tracker\n",
    "    tracks = tracker.update(boxes_xyxy)\n",
    "\n",
    "    # Maintain trajectory\n",
    "    for oid, tr in tracks.items():\n",
    "        cX, cY = tr.centroid\n",
    "        if oid not in tracks_history:\n",
    "            tracks_history[oid] = deque(maxlen=TRAJ_MAX_POINTS)\n",
    "        tracks_history[oid].append((cX, cY))\n",
    "\n",
    "    # Optional counting\n",
    "    if ENABLE_COUNTING:\n",
    "        for oid, tr in tracks.items():\n",
    "            cX, cY = tr.centroid\n",
    "            if oid in prev_centroids:\n",
    "                prevY = prev_centroids[oid][1]\n",
    "                if prevY < line_y <= cY:\n",
    "                    count_in += 1\n",
    "                elif prevY > line_y >= cY:\n",
    "                    count_out += 1\n",
    "            prev_centroids[oid] = (cX, cY)\n",
    "\n",
    "    # Draw overlays\n",
    "    if ENABLE_COUNTING:\n",
    "        cv2.line(frame, (0, line_y), (width, line_y), (0, 255, 255), 2)\n",
    "        cv2.putText(frame, f\"IN: {count_in}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 200, 0), 2)\n",
    "        cv2.putText(frame, f\"OUT: {count_out}\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 200), 2)\n",
    "\n",
    "    for oid, tr in tracks.items():\n",
    "        x1, y1, x2, y2 = tr.bbox\n",
    "        cX, cY = tr.centroid\n",
    "        color = id_color(oid)\n",
    "        # bbox and id\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "        cv2.circle(frame, (cX, cY), 3, (0, 255, 255), -1)\n",
    "        cv2.putText(frame, f\"ID {oid}\", (x1, max(0, y1 - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "        # trajectory polyline\n",
    "        pts = tracks_history.get(oid, None)\n",
    "        if pts and len(pts) > 1:\n",
    "            for i in range(1, len(pts)):\n",
    "                pt1 = pts[i - 1]\n",
    "                pt2 = pts[i]\n",
    "                cv2.line(frame, pt1, pt2, color, TRAJ_THICKNESS)\n",
    "\n",
    "    # FPS + progress\n",
    "    elapsed = time.time() - start_time\n",
    "    fps_text = f\"FPS: {frame_count / elapsed:.1f}\" if elapsed > 0 else \"FPS: --\"\n",
    "    cv2.putText(frame, fps_text, (width - 200, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (50, 220, 50), 2)\n",
    "    prog_text = f\"{frame_count}/{total_frames}\" if total_frames else f\"{frame_count}\"\n",
    "    cv2.putText(frame, prog_text, (width - 200, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (200, 200, 50), 2)\n",
    "\n",
    "    writer.write(frame)\n",
    "\n",
    "pbar.close()\n",
    "cap.release()\n",
    "writer.release()\n",
    "\n",
    "print(f\"Saved full-frame with trajectories: {out_path_traj}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cbde28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center-crop run with trajectory visualization\n",
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "\n",
    "# Ensure model is loaded\n",
    "try:\n",
    "    _ = model\n",
    "except NameError:\n",
    "    last_error = None\n",
    "    model = None\n",
    "    for candidate in MODEL_CANDIDATES:\n",
    "        try:\n",
    "            model = YOLO(str(candidate))\n",
    "            print(f\"Loaded model: {candidate}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            last_error = e\n",
    "            print(f\"Failed to load {candidate}: {e}\")\n",
    "    assert model is not None, f\"Could not load any model from {MODEL_CANDIDATES}. Last error: {last_error}\"\n",
    "\n",
    "# Open video\n",
    "cap = cv2.VideoCapture(str(VIDEO_PATH))\n",
    "assert cap.isOpened(), f\"Cannot open video: {VIDEO_PATH}\"\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "src_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "src_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Crop size (adjust as needed)\n",
    "CROP_W = min(1920, src_w)\n",
    "CROP_H = min(1080, src_h)\n",
    "\n",
    "x0 = max(0, (src_w - CROP_W) // 2)\n",
    "y0 = max(0, (src_h - CROP_H) // 2)\n",
    "x1 = min(src_w, x0 + CROP_W)\n",
    "y1 = min(src_h, y0 + CROP_H)\n",
    "\n",
    "out_w, out_h = (x1 - x0), (y1 - y0)\n",
    "\n",
    "out_path_traj_crop = OUT_DIR / \"_tmp_center_crop_traj.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "writer = cv2.VideoWriter(str(out_path_traj_crop), fourcc, fps, (out_w, out_h))\n",
    "\n",
    "# Tracker and trajectory store\n",
    "tracker = CentroidTracker(max_disappeared=MAX_DISAPPEARED, max_distance=MAX_DISTANCE)\n",
    "tracks_history: Dict[int, deque] = {}\n",
    "TRAJ_MAX_POINTS = 40\n",
    "TRAJ_THICKNESS = 2\n",
    "\n",
    "count_in, count_out = 0, 0\n",
    "line_y = int(LINE_Y_FRACTION * out_h)\n",
    "\n",
    "\n",
    "def id_color(oid: int) -> Tuple[int, int, int]:\n",
    "    r = (37 * oid) % 255\n",
    "    g = (17 * oid + 85) % 255\n",
    "    b = (97 * oid + 170) % 255\n",
    "    return int(b), int(g), int(r)\n",
    "\n",
    "predict_kwargs = dict(\n",
    "    conf=CONF_THRES,\n",
    "    iou=IOU_THRES,\n",
    "    imgsz=IMG_SIZE,\n",
    "    classes=[PERSON_CLASS_ID],\n",
    "    agnostic_nms=globals().get(\"AGNOSTIC_NMS\", False),\n",
    "    max_det=MAX_DET,\n",
    "    augment=True,\n",
    "    verbose=False,\n",
    "    device=globals().get(\"DEVICE\", 0),\n",
    "    half=True,\n",
    "    amp=True,\n",
    ")\n",
    "\n",
    "prev_centroids: Dict[int, Tuple[int, int]] = {}\n",
    "\n",
    "# Progress bar\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "if total_frames <= 0:\n",
    "    total_frames = None\n",
    "pbar = tqdm(total=total_frames, desc=\"Center-crop w/ traj\", unit=\"frame\")\n",
    "\n",
    "start_time = time.time()\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame_count += 1\n",
    "    pbar.update(1)\n",
    "\n",
    "    crop = frame[y0:y1, x0:x1]\n",
    "\n",
    "    results = model.predict(crop, **predict_kwargs)\n",
    "    boxes_xyxy: List[Tuple[int, int, int, int]] = []\n",
    "    if len(results) > 0:\n",
    "        r = results[0]\n",
    "        if r.boxes is not None and len(r.boxes) > 0:\n",
    "            xyxy = r.boxes.xyxy.cpu().numpy().astype(int)\n",
    "            confs = r.boxes.conf.cpu().numpy()\n",
    "            clss = r.boxes.cls.cpu().numpy().astype(int)\n",
    "            for (x1b, y1b, x2b, y2b), c, k in zip(xyxy, confs, clss):\n",
    "                if k == PERSON_CLASS_ID and c >= CONF_THRES:\n",
    "                    x1b, y1b = max(0, x1b), max(0, y1b)\n",
    "                    x2b, y2b = min(out_w - 1, x2b), min(out_h - 1, y2b)\n",
    "                    if x2b > x1b and y2b > y1b:\n",
    "                        boxes_xyxy.append((x1b, y1b, x2b, y2b))\n",
    "\n",
    "    tracks = tracker.update(boxes_xyxy)\n",
    "\n",
    "    # Update trajectory deque\n",
    "    for oid, tr in tracks.items():\n",
    "        cX, cY = tr.centroid\n",
    "        if oid not in tracks_history:\n",
    "            tracks_history[oid] = deque(maxlen=TRAJ_MAX_POINTS)\n",
    "        tracks_history[oid].append((cX, cY))\n",
    "\n",
    "    if ENABLE_COUNTING:\n",
    "        for oid, tr in tracks.items():\n",
    "            cX, cY = tr.centroid\n",
    "            if oid in prev_centroids:\n",
    "                prevY = prev_centroids[oid][1]\n",
    "                if prevY < line_y <= cY:\n",
    "                    count_in += 1\n",
    "                elif prevY > line_y >= cY:\n",
    "                    count_out += 1\n",
    "            prev_centroids[oid] = (cX, cY)\n",
    "\n",
    "    if ENABLE_COUNTING:\n",
    "        cv2.line(crop, (0, line_y), (out_w, line_y), (0, 255, 255), 2)\n",
    "        cv2.putText(crop, f\"IN: {count_in}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 200, 0), 2)\n",
    "        cv2.putText(crop, f\"OUT: {count_out}\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 200), 2)\n",
    "\n",
    "    for oid, tr in tracks.items():\n",
    "        x1b, y1b, x2b, y2b = tr.bbox\n",
    "        cX, cY = tr.centroid\n",
    "        color = id_color(oid)\n",
    "        cv2.rectangle(crop, (x1b, y1b), (x2b, y2b), color, 2)\n",
    "        cv2.circle(crop, (cX, cY), 3, (0, 255, 255), -1)\n",
    "        cv2.putText(crop, f\"ID {oid}\", (x1b, max(0, y1b - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "        pts = tracks_history.get(oid, None)\n",
    "        if pts and len(pts) > 1:\n",
    "            for i in range(1, len(pts)):\n",
    "                cv2.line(crop, pts[i-1], pts[i], color, TRAJ_THICKNESS)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    fps_text = f\"FPS: {frame_count / elapsed:.1f}\" if elapsed > 0 else \"FPS: --\"\n",
    "    cv2.putText(crop, fps_text, (out_w - 200, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (50, 220, 50), 2)\n",
    "    prog_text = f\"{frame_count}/{total_frames}\" if total_frames else f\"{frame_count}\"\n",
    "    cv2.putText(crop, prog_text, (out_w - 200, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (200, 200, 50), 2)\n",
    "\n",
    "    writer.write(crop)\n",
    "\n",
    "pbar.close()\n",
    "cap.release()\n",
    "writer.release()\n",
    "\n",
    "print(f\"Saved center-crop with trajectories: {out_path_traj_crop}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7caf22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "panogeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
